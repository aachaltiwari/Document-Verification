{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e872a817-43a4-4e8e-a734-d382c19ec867",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85acea23-ba49-4726-8bb4-3f71666fcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install evaluate\n",
    "# !pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21e3946-0b86-42b6-9c9a-f7f3db0a8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TrOCRProcessor, VisionEncoderDecoderModel, AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6805f-84bb-4275-a492-d1564410bf92",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9776255-9859-4f28-9cb2-15b862ac888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "csv_path = \"ocrdataset/dataset/data.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0bc07-571a-45f8-8077-1d5c9a975838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "# Load the processor, tokenizer, and model\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff211eb-a8c1-4f4a-bed3-72ce229a8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'ट', '्', 'र', 'क']\n"
     ]
    }
   ],
   "source": [
    "word = \"साल: 0224 महिना: 20 गते: 987651287445! तपाईको नाम क हो ?\"\n",
    "word = \"त्रुच्क\"\n",
    "word = \"क्षमता\"\n",
    "word = \"ज्ञानी\"\n",
    "word = \"प्रकाश\"\n",
    "word = \"ट्रक\"\n",
    "tokens = processor.tokenizer.tokenize(word)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e29830-a53c-4fcd-a5b8-ebb8411e819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "list_nepali_digits = ['़','ँ','ं','ः','।','ृ','्',\n",
    "                      'अ','आ','इ','ई','उ','ऊ','ऋ','ऌ','ए','ऐ','ओ','औ',\n",
    "                      'क','ख','ग','घ','ङ','च','छ','ज','झ','ञ',\n",
    "                      'ट','ठ','ड','ढ','ण','त','थ','द','ध','न',\n",
    "                      'प','फ','ब','भ','म','य','र','ल','व','श','ष','स','ह',\n",
    "                      'ा','ि','ी','ु','ू','े','ै','ो','ौ',\n",
    "                      'ॐ','ॠ',\n",
    "                      '०','१','२','३','४','५','६','७','८','९'\n",
    "                     ]\n",
    "\n",
    "print(len(list_nepali_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5f789a-9830-4396-a0d5-b933588be500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ः', 'ऊ', 'ऋ', 'ऌ', 'ऐ', 'ङ', 'ञ', 'ॐ', 'ॠ', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९']\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "unknown_characters = []\n",
    "for i in list_nepali_digits:\n",
    "    token_id = processor.tokenizer.convert_tokens_to_ids(i)\n",
    "    # print(i, token_id)\n",
    "    if token_id == 3:\n",
    "        unknown_characters.append(i)\n",
    "\n",
    "print(unknown_characters)\n",
    "print(len(unknown_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4d98b3-dff1-49e4-b58a-4ace03c94da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "¶ 62875\n",
      "§ 62919\n",
      "† 62825\n",
      "‡ 63209\n",
      "‰ 63463\n",
      "¤ 63210\n",
      "¢ 36728\n",
      "£ 37118\n",
      "¿ 60576\n",
      "∞ 63051\n",
      "∑ 63542\n",
      "∂ 63043\n",
      "∇ 63785\n",
      "∆ 63574\n",
      "⌘ 63271\n",
      "≈ 62835\n",
      "≠ 63384\n",
      "¢ 36728\n",
      "¥ 63254\n"
     ]
    }
   ],
   "source": [
    "rare_symbols = [\n",
    "    '¶', '§', '†', '‡', '‰', '¤', '¢',  '£', '¿', '∞', '∑',  '∂', '∇', '∆',\n",
    "    '⌘', '≈', '≠', '¢', '¥'\n",
    "]\n",
    "print(len(rare_symbols))\n",
    "\n",
    "for i in rare_symbols:\n",
    "    token_id = processor.tokenizer.convert_tokens_to_ids(i)\n",
    "    print(i, token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10cbd20-1c4b-4fd9-82d5-10c6651026ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¥'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = processor.tokenizer.convert_ids_to_tokens(63254)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c476311-0dfd-4e79-a88c-25567cb565f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ः': '¶',\n",
       " 'ऊ': '§',\n",
       " 'ऋ': '†',\n",
       " 'ऌ': '‡',\n",
       " 'ऐ': '‰',\n",
       " 'ङ': '¤',\n",
       " 'ञ': '¢',\n",
       " 'ॐ': '£',\n",
       " 'ॠ': '¿',\n",
       " '०': '∞',\n",
       " '१': '∑',\n",
       " '२': '∂',\n",
       " '३': '∇',\n",
       " '४': '∆',\n",
       " '५': '⌘',\n",
       " '६': '≈',\n",
       " '७': '≠',\n",
       " '८': '¢',\n",
       " '९': '¥'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepali_to_english = {}\n",
    "for i in range(19):\n",
    "    nepali_to_english[unknown_characters[i]] = rare_symbols[i]\n",
    "\n",
    "nepali_to_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a45c89-1993-409f-b3b6-1fa1dcfdbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Nepali digits with English digits in the 'text' column\n",
    "df['text'] = df['text'].replace(nepali_to_english, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95350e92-4f3e-4f13-944c-bed9c5a5c994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/preprocessed_images/0.png</td>\n",
       "      <td>थिए।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/preprocessed_images/1.png</td>\n",
       "      <td>थिए। दा§दले स्वर्गदूतले</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/preprocessed_images/2.png</td>\n",
       "      <td>∂⌘-∞⌘-∂⌘-∂⌘∂≈∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/preprocessed_images/3.png</td>\n",
       "      <td>∞∂-∞¢-∂∂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/preprocessed_images/4.png</td>\n",
       "      <td>पनि छोराहरू थिए।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_file                     text\n",
       "0  dataset/preprocessed_images/0.png                     थिए।\n",
       "1  dataset/preprocessed_images/1.png  थिए। दा§दले स्वर्गदूतले\n",
       "2  dataset/preprocessed_images/2.png           ∂⌘-∞⌘-∂⌘-∂⌘∂≈∞\n",
       "3  dataset/preprocessed_images/3.png                 ∞∂-∞¢-∂∂\n",
       "4  dataset/preprocessed_images/4.png         पनि छोराहरू थिए।"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b53eebf-dc89-4971-ac49-744070d1f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac7e6f-2b00-4044-b3c6-333cc2f6c266",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bd3262b-102d-45d9-b200-ef1d59421613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['image_file'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "796e9a63-bc78-49ff-b993-4a031408f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IAMDataset(root_dir='ocrdataset/',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='ocrdataset/',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d54be301-4971-4c9e-8416-9f4f78e34fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56000\n",
      "Number of validation examples: 14000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8df0969d-a7db-4cb3-a2e3-680e4611f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71cb935a-c60b-46bb-90bb-476109f47b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAzAGUDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1aiiigAooooAKKKKACiiigAooqrey30QRrS2hmXkuHcq30FAFqiq1hfRajZpcxBlBJDI3VGBwQferNABRRRQAVHPJJFGrRW5nJOCBIF2/nUlZ2vXs+naFeXdtF5k8ceUHocgZ98daAKeq+IpNI+yC40uUm6mEEeyVT8x6ZxS6trt1pC27TaWX8+UQrsnU4Y9O1QX1rpaWOm3+u6nLIsUscsUsk3ymQ9MACofGhHk6K+7ganCR79aANxpdTC5FhD/4Ej/Csq913VbPWrTT30YFbrdsuBOCgIGcEgda3Wu4IruG2kbE02fLXH3sdRXIeJZdV0rVtM1F5YbyNpfs8MDLsWKVuA/+1xQB0nm6sf8Al1tR9Zj/APE0yW41eKB5RaWjlUJ2ea3b3xVJr7UdP8S2NjPMLyK+jdmCxhfJZR1GP4T71R0yBNQ1DXG1i6nlkjmYC2ZyqpCBxtUHkEd6ANbS9Q1fUNOhu3srOEyqCI2kY8evFXmGqjkxWS/XzKx4NZtb/wAH3d14fLKsEbRxZQjYVxnGfQZqjf6HdahpelzeH76ZDIyfaJ1mJLREfMTz14o6gLpt3qFr8QLvS2+ymGeIXcgGQF5wcD1NdecZ46Vw2n6Vb6T8UGW3aUiXTASZG3EkNjOa7mgAooooAKRlV1KsAVIwQe4paKAOb1jwTpeq6e9r++jXd5kaCQ+WrjocelaN3pCanY2MV78sttJHKfLPG5RitOigCrf6db6nbiC4DABtyOhw0beqnsaqnREmu7a4vbqe8+zNvhWXG0P/AHiB1NalFAFYWFsNSfUNjG6eMRFy2QFHYDtTbzS7K/kWS5gVpFG0OCVbHpkdRVuigDM0XRo9Et57O3KmzaTfDHySmR8wOe2aNJ0WLRnuhazy/Z55PMFu33Yz321p0UAVv7PtP7UGp+UftghMAfdxtJz0/OrNFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGUAAAAzCAIAAAAByVFyAAANyElEQVR4Ae2aWXcUR5aAI9eq3Kqy9k0qCe0CwdGGsAxuGoPBdmN3u9uznDPzMD9p5mXOPMw58+Dudrt9xu6BBrNjCzCbhQRYuwSSSlLtS2blUrlPloWQoLstlex5MFP3ITMy4t4bkV/cuBGpEjQ5OQnqsmMC8I4164pVAnVetcVBnVedV20EatOux1edV20EatOux1edV20EatOux1ed14sEGv2tXixMeSi/0/Viy26e0N0Y/aRs5jKzj+7cMyUlHNxD7fV2NfrySXXXb/Dq8+Kf8vP5pM6hifQMWOPng8ED/W9QANodsleflzvmfw3dhxsBU+bvLU4+ejBakTynfzEkcvIukL36vBqa2x1Ow0YTDsaVb5hbnDa9+HBgpdHFsLvg9ervj9nU9DqXVEZoj/uOdHUpheKjsUkWDdV5bU/A39rd1NG88Hh58enE9tp/ofHqx9dLr9zew+6LRMv68uj8NIawL7Vu+/gTzl++xqZ8Ysl+wxZXUwaoAp+0y3TYSRoEDlmS4cRMS9QQgGQFXmfdrGzmMD2YSGeALwyhxOL86P1YpLulPcaSTsq5ks5tC8tW2BEvNh7TuTzkIHQBNnVBheEQyZQKeZXFnEUYCRCFVBlBMaAhDpiTXZhRVhEMJXTRIDBFN7GSYnrZYjkHSIrJyCqDKBXM66wUERjSKziPS1YFQSwDo2hONmiioqqYICgoamoODEC8x6BzvGi6Ra/hAlkuQzlMHAtas7MJE1K5FXkCuW8EMEtkXF6oNC3pnKKRALEQyixjBGZJakbz+GVINTTDVVJ1X7q8whcLC09ShsKnWg+74syRvk4AdnSa3Z4XHQgs3ZxMCfMq55IZxS2WLRc2XlQkAsCqVeBFziw3eglR9OAkQSCaUTa5bCKHaIgKUZbl8FEmDuE8ChFuGBINVeAkknLwuOXKkYa+VkgWk9knS7i3eW9nEwncsFGuOBVYoXgXisomgaOEAAG2yJUJFjNl2saqB/IhbrXCp+R8eWF+IdHIMq2dHc5ohMxSkEOmYbcqcgRGisDg0rjHrWFYRoYgGWM0HacdCG3HUzg6uTzL3ZVija1eqEVyhuIhdyaR2jbEtudVAdqi/DSdy8NEBpU85bJklTE3ghFOaXGi9OfJK64irJw61bYfcUqmiFKkW1QE5Mbl2wxu7eseHmIaqTDLwRIkuL1wRTcITi+6fRGzHIwj4qo8Mzb75Kv7D/Z1goPHDwzF2jXEMgmXu1KpMAaGk7DhQsoC5EAMjSBgGVCIiTmBouAALKwZVx/xldE5al9n//C77fEoQFVDEUWAQaYDkGJQjgC3YPdZdLkcpaLOOBkINxVaqRRoE7t7ZZzyC/ubB/ceGHDDjtXE2rawdrQeGQvrPzyA5oQKTRo8Tpu67NdgnVI1Q+Rvgi/LMnAMd3X3Hes2Nb9hANzIPJ5eGL07Y+ZWhvpa3jk6XIAcDObQcHuidX5pMeQ5li7OdXnDKoQtDHZonpBVkRAT9Lb0haLB54N2Py8xzo3iRgG3cYH2TmChAygHDQ8Ot7QEDKAB+9TuoKl1bYsSnDxQ7DrYJq4jGJCMMrDPqLKdhFqxNoRWNIyh4mFBzwuljR62u28fX8Vc1nZiAAISLBRWKjCASrAFZAyApriX9RKWReJtPjGlA1CNZ/vbLOaOen1EyRFk2HiCt4esiNVxVP3YYsOyr9OFqrL9VfJPvzlNWtz4xLfFbLFxC6+q6vcKGfVqDyFUMhpCzF8qslEPoWmcyDgsjUD0tQ0kOO7VYGkqVzA4kXUigC8Dz44y13oX2/N6aShNMTKTUmkPbZrc7LhYSSuVZqfHsOdtU6yKWF7KZk3I6f0O1GbLXymZmkAqMCiUoYrwV5q3VDUyrsl8upTM5VCeLDgroyPXvhnTykheVDaD8Tt9hEJv3LovicukSfEVmdID3YPtHkAvrk2NJ79Wssm5qamVdMnnbYAMcksP2xdr46U5jQufPRadvFPCTVpcmlkCDlfUJGXJ494yxxplpuGyghACTIb+9hjiocanxaUrl27fG52TPa7Wvd3RULui5LniWolPVRyhnkA0VS6uO2Abyatn73x64TKfS/d1DoYDlGxAYYA529rZUHRrJ5QfXDr/+YNrC/FoiIp1mtzC2Ny3I5O3SEBy2XmVdIZ87HIqgwl5AMmmy55pz1bz7y/XwMuNsx/98dP51ek9/kaqJRTUde+e0HsffsC64M5okDM2o6MoaBCmw5rsK6pgC8fnQ/E20jPzU6NXEk9Tc9+eu8mryff+5R+oQCQ5P/Ygl0gvzk8+LOBM+dChUycP9hblapCuLFX+8OevHk1d27//+PBQr39PkGRIyzyOsy69vBnFnobYnfMjE7fnyJbYkbffICN7zFy8hN65/Nl/Z0Wjqa35SGtPf18vXYksTmQkHc8XHH7/83FtX9gpr6Ywff7K+bGlG3G24/WTb7r9jSFI0xEDlk08SubXuK1d0YzDpeIC6cxgOLu1YaPMr6rL97Mr2UXWzrqYlTOJvIJcv3tu5f6KRXDdDa2HBrsejly69ckXPqHS/+t3S2sJY7nM5XKtnqZfvTXcf6w3X+BtZxACtsKyazh1bTozsaTwf9dy2OeKAlEBhH9wf8uZi4i8yh/tHXz/n9+PkKyKgMjEVYIXQrCdXmuQnfJKJUt3z98rlAqnj70ecruAxm0SehGW3bmmu9FgWM0laa0MwOaKDHoCgNQzq0XdVBt67f0t4oODoWbPn658cfPj3zowX0MzMzTw5pGBk17ScHjYP/3rf31893OiPxDBwzxTJFvUTqqje2/fOqznb4lYXjzgkHNJu4aqsFIJCVrAH9nsl2LcIbaR6QkO9hxEVTOrFvyE6cfgZReQaJR97mgHhZ1+PyZKRokXMYPw+c2tbmHCLSJ6gMC2Vjp0ERREw4HDQnXjXxfEVK6NXS9NPTvmhGiGpiMKiRw//Mv333xDVZBiJtV+8NDbR39ZMbk1QWgLNRhuZHUyVVqtLreA4vVK8BMBFuXCusMARa+VCrcv3rj41X+MXz/LQNWDBE67GtqjIhvKrJX83mcbH8s62yMNJ08c7znQvm6LApdmamyJ1vM7+gxat7KvO+UVjPjir7VhGJMVi23e1nV7KoiOXPif3//7J3+8eF6yqn9jWhcINTAfHkjjRecmRyWrL91cujcxtqH17J4przV0DvgiMB4GLaFwka+eM2yxdL1iYXQ8RILqoQyJ+wPekLqaHv/6CUQhdo0BS48eTZ0/e+7f/vO3lz67lOCqx5T82tLepmbUFL8Zm1hMzrV+l5zISNcH//jW8VND/EaSlWBUUqyiS9GtF6bf9vD9slNeoCL0dR/qYJnEzdkvU1OMoxrty3PGtQe3/3Dmd7cuXM+vpp/3ZEoOS5IKaBFIz3Y3u0n3qctW6dr4QjazQDW/kGMZzSErFpIHCGStOwkEyIXpb1kH2be3+8C+uF2J6dKJt0+G2vx3H42cu3AlK+qirrK4njLL1qJOewjEyz6zDTLtgfjc3Oy1i6MzyRm70shmu1uHeiK96wr21U+hLOLxqoKO/x/xAmBPV0dn775MKfvgd59cvnwtl8jkk7PJlXw2t7I4X9Iq9nn1mTAoqKiEy8nC5uYvC7FAx7GuDhhOf/TRublbD2MB37p2gz86vjxRkVmmoZUNV08GEU9oZPTB+TtXSJo6+rMhbgP64MDAr0980BB1T0zOf/3FmdvnbpSyRbTEeRr87e39MW9k3aGpovv7ThFx6PzX586evaOZXLghUuBSy9mFZ+MDIGfa24zDoEFZwNoC1fjdoew039vuYE3d92Y/5vI9Sa/cezyaKLhoCNhZqckR8DdpsFwIRw+m1qpRploqGVGTSQtzbPrPF7NH3hrWHMQXd298/PuL/NpqJB7VWGx25PGtSzebvcTR93/WtKdZyHKmJKRmlmUBfuP13lhDv7WRsFayXM/AsUiQvTU6vsynZoqcgereQz3YvLxiSMU8X12l34ltl0sf+nz23FdfnSFI6NR71ZFuNFbvQZxm20OVtO6KeRTE/srYqWy+z04s3LD/6NvRxqcrSyvTcEYyJejYe6enIgdQcaEAC6L1LMQon7/Fu9cnlEkHsdVtrqQf6R/WceKbe1fvTM+H55LAqwhLCE2Tfb1H3jkxmM1Wd920Irb293wYjcQDLc9hrfspFudgmn7nwxOlp5BAc24eXykvjoxcTidXnzycOXli6GmqZGtmykLX8K9eyyH3b376eHS2v/tAsKlh3cP6NdZIv/PWEU6q7O+IJVLVOd6hQLv7f0wfHVPQSlEtsyR57/7YnZHrDE7//anjwFtdZfYvow9Hpw0y37bnNRVUd7etwjhd2WQiJ2Q4BcEoguEhKopFw1HBPittyB5/KM1rtB/NrGU26v7mPbU6c/XmPU+IPn36N6awmY9KHDwz96WoyIcP/dyBOl6yj3iDPJBZh2c1ufxS0/c87pLXVo8Ew567ciaxtPDusWPd+w7lMttPVyDSTivG08KTrX52XcYR8OD6LRkjfn68V5Re2MHIqE8Xyyq3mUZ33cu64Y/Ay3akYciTiUR7cyOMb54qfuDIdm7uDwQLckZR3CF3oJCb37nhLjR/HF52x75YQ351ZRcj+GmZvBC9P2To/x9g2Xx+NF4/hPVPyLbOq7bJqvOq86qNQG3a9fiq86qNQG3a9fiq86qNQG3a9fiq86qNQG3a9fiqjdf/AulLRwQD2itZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=101x51>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['image_file'][0]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06c8a3c-dc62-48f3-8a5a-64aed3301a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "दा§दले\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c148c062-2153-48d3-b75e-7fc297ce346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9ea14-6a78-4b82-a416-6aff0af3e271",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e30b310-b908-43ce-9d8e-f92af1dd32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02ee320b-9f68-45cf-89c3-38782007ac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.48.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTSdpaAttention(\n",
       "            (attention): DeiTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(64044, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db93dd61-f692-4077-a394-29c3422b7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0467ec14-42a4-4e9b-abe5-866a2e254e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51769826-8ece-4e54-9219-92838a27ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "babbf68b-86a3-477f-b32d-3cf5ac5a2919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3ed299e0af465e94c92d9c72fcb8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m    optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m    optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m    train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss after epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(1):\n",
    "   # train\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for batch in tqdm(train_dataloader):\n",
    "      # get the inputs\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "   # evaluate\n",
    "   model.eval()\n",
    "   valid_cer = 0.0\n",
    "   with torch.no_grad():\n",
    "     for batch in tqdm(eval_dataloader):\n",
    "       # run batch generation\n",
    "       outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "       # compute metrics\n",
    "       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "       valid_cer += cer \n",
    "\n",
    "   print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "\n",
    "model.save_pretrained(\".\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8eb56-2f3f-4a29-8dd1-9998ce4cc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model/part_test/nepali_ocr_model\")\n",
    "processor.save_pretrained(\"model/part_test/nepali_ocr_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b19e2-f0a4-4d2c-950c-f599d3e4962b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
